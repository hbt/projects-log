:uri-asciidoctor: http://asciidoctor.org
:icons: font
:source-highlighter: pygments
:nofooter:

++++
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-90513711-1', 'auto');
  ga('send', 'pageview');
</script>
++++

link:index[Home]

== rewriting-video-converter-project-using-docker-siblings-75




## Iteration 7

Now that we have all the utilities dockerfied and easily callable using docker compose with entrypoints. 

We can rewrite the video converter script to use the dockerfied utils. 

Since we moved from one docker image to multiple images, it means we scale up the components as needed.
Example: we can run youtube-dl on one vm but use ffmpeg on different vms since it takes 1 CPU per file


The problem with docker components is how to link them. We have 3 options:

- docker siblings
- docker in docker
- docker duh


### Docker siblings 

Install docker in the dockerfile
Mount the docker socket and use docker. 

This does not make more secure. docker will run on the host and mounting volumes uses the host's path instead of the container


### Docker in Docker

Run it with `--priviliged` flag and put docker in docker. 

- You have to start the docker service each time which defeats the purpose of small scripts with many dependencies.
- It takes more disk space since you have to pull the dependencies in a separate docker volume

### Docker 

Put everything in the image. Basically keep what we have done in previous iterations. 

In the case of something like the video-converter project, it's fine. However, since the goal here is to build a framework to add/publish/run functions easily, it's worth skipping this option.

We don't need to break the image to scale up either. We can simply run the same image and use different entrypoints. 



*None of these options will work remotely since we cannot allow any user to mount/access everything in the VM*


## Iteration 8


### Issues

- user id and docker user mapping

Docker user mapping is one of those hidden issues. The docker container user id must match the host's user id in order to work. 
Everything worked fine on my desktop and vms because I'm the only user and therefore have id 1000
But it failed when deployed on prod. 

Potential solutions:

- creating user in container with same user id and group id https://gist.github.com/renzok/29c9e5744f1dffa392cf
- configuring the docker daemon itself https://success.docker.com/KBase/Introduction_to_User_Namespaces_in_Docker_Engine
- look into chown like utilities https://github.com/ncopa/su-exec/blob/master/README.md


The whole point of mapping users is for the files created/modified by the container to not have root permissions.
for now since everything runs in a wrapper anyway, I could easily chown before/after



### utilities as entrypoints

this has been modified by using helpers scripts. 
It means you cannot simply download the docker image and docker compose file but you need the whole git repo in order to mount the source code as a volume.

This makes development much easier (by not having to build a docker image to execute new code) but makes production deployment require an extra resource (the git repo (ssh keys, git updates etc.))




### writing helpers

This has been a breeze. With the dependencies and configuration in docker, the code can be broken into small functions and shared easily, refactored easily etc.
It is not strongly typed yet and that makes exploration/discovery a problem.

It all follows the same pattern and that makes it easy to grasp. Limiting the number of patterns to learn, understand and manipulate accelerates things.


### multiple scripts to ease development

- in dp-scripts (runs in containers)
- in scripts (bash only runs locally)


### communication between processes across containers

Other than an API gateway, this is possible by mounting the docker socket (aka docker siblings).
Then running the docker-compose utility. 

Sharing data is best done through shared volumes. Example:

```
version: '3.4'

services: 
  youtube-dl:
    image: hbtlabs/youtube-dl
  video-converter:
    build:  
      context: . 
      labels:
        com.functionshub.dpm: "video-converter"
    image: hbtlabs/video-converter
    environment:
      - USER_PWD=$PWD
      - USER_HOME=$HOME
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#      - $HOME/docker-files/src/php/config/php.ini:/etc/php/7.0/cli/php.ini
      - $PWD:/mounted
      - shared_data:/shared_data
      - $HOME/docker-files:/home/user/docker-files
      - /etc/localtime:/etc/localtime:ro
      - $HOME/.dpm:/home/user/.dpm
#      - $HOME/docker-files/secrets/secrets/ssmtp.conf:/etc/ssmtp/ssmtp.conf
    working_dir: /mounted      
    
volumes: 
  shared_data:
    external: true
```    

```
version: '3.3'

services: 
  youtube-dl:
    build:  
      context: . 
      labels:
        com.functionshub.dpm: "youtube-dl"
    image: hbtlabs/youtube-dl
#    entrypoint: /usr/local/bin/youtube-dl
    volumes:
      - $PWD:/mounted
      - shared_data:/shared_data
      - $HOME/docker-files:/home/user/docker-files
    working_dir: /mounted      
      

volumes: 
  shared_data:
    external: true
```


Running docker compose will run on the host and not the container. Therefore paths, environment variables have to be adjusted if called from a container.
Also, possible to use docker-compose fragments to overwrite config in other images and load partials using `-f`

This is not needed at the moment since I can simply put everything in the same image and I'm not sure I want to go in that direction; I'd rather work on an API gateway.
